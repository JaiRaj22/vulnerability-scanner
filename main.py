import streamlit as st
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from selenium import webdriver
from selenium.webdriver.edge.options import Options
from selenium.webdriver.edge.service import Service as EdgeService
from webdriver_manager.microsoft import EdgeChromiumDriverManager

def print_red(text):
    st.error(text)

def print_green(text):
    st.success(text)

def crawl(url):
    """
    Crawl the given URL and return a list of all found links.
    """
    links = set()
    try:
        # Set up Edge in headless mode
        options = webdriver.EdgeOptions()
        options.add_argument('--headless')  # Run Edge in headless mode
        options.add_argument('--disable-gpu')  # Disable GPU hardware acceleration
        options.add_argument('--no-sandbox')  # Bypass OS security model, required on many Linux distros
        options.add_argument('--disable-dev-shm-usage')  # Overcome limited resource problems

        # Initialize the Edge driver
        driver = webdriver.Edge(service=EdgeService(EdgeChromiumDriverManager().install()), options=options)
        driver.get(url)
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        driver.quit()

        # Find all anchor tags
        for a_tag in soup.find_all('a', href=True):
            link = urljoin(url, a_tag['href'])
            if urlparse(link).netloc == urlparse(url).netloc:  # Only crawl the same domain
                links.add(link)
                st.write(f"Found link: {link}")  # Debug print to check found links

        # Find all form actions
        for form_tag in soup.find_all('form', action=True):
            link = urljoin(url, form_tag['action'])
            if urlparse(link).netloc == urlparse(url).netloc:
                links.add(link)
                st.write(f"Found form action: {link}")  # Debug print to check found form actions

        # Find all script sources
        for script_tag in soup.find_all('script', src=True):
            link = urljoin(url, script_tag['src'])
            if urlparse(link).netloc == urlparse(url).netloc:
                links.add(link)
                st.write(f"Found script src: {link}")  # Debug print to check found script sources

        # Find all link tags (e.g., stylesheets)
        for link_tag in soup.find_all('link', href=True):
            link = urljoin(url, link_tag['href'])
            if urlparse(link).netloc == urlparse(url).netloc:
                links.add(link)
                st.write(f"Found link tag: {link}")  # Debug print to check found link tags

    except Exception as e:
        st.error(f"Error crawling {url}: {e}")
    return links

def check_sql_injection(url):
    """
    Check for SQL Injection vulnerability.
    """
    payloads = ["' OR '1'='1", "' OR '1'='1' --", "' OR '1'='1' /*", "' OR '1'='1' #"]
    for payload in payloads:
        test_url = f"{url}?id={payload}"
        try:
            response = requests.get(test_url, verify=False)
            if any(error in response.text.lower() for error in ["syntax error", "mysql", "sql", "database"]):
                print_red(f"SQL Injection vulnerability detected at {test_url}")
                return
        except requests.RequestException as e:
            st.error(f"Error checking SQL Injection for {url}: {e}")
    print_green(f"No SQL Injection vulnerability detected at {url}")

def check_xss(url):
    """
    Check for XSS vulnerability.
    """
    payloads = ["<script>alert('XSS')</script>", "<img src=x onerror=alert('XSS')>", "<svg onload=alert('XSS')>"]
    for payload in payloads:
        test_url = f"{url}?q={payload}"
        try:
            response = requests.get(test_url, verify=False)
            if payload in response.text:
                print_red(f"XSS vulnerability detected at {test_url}")
                return
        except requests.RequestException as e:
            st.error(f"Error checking XSS for {url}: {e}")
    print_green(f"No XSS vulnerability detected at {url}")

def check_open_redirect(url):
    """
    Check for Open Redirect vulnerability.
    """
    payloads = [
        "http://evil.com", 
        "//evil.com", 
        "/\\evil.com", 
        "/%5Cevil.com", 
        "/%2Fevil.com", 
        "/%252Fevil.com", 
        "///evil.com", 
        "////evil.com"
    ]
    for payload in payloads:
        test_url = f"{url}?next={payload}"
        try:
            response = requests.get(test_url, verify=False, allow_redirects=False)
            location = response.headers.get('Location')
            if response.status_code in [301, 302] and location and (payload in location or urlparse(location).netloc == "evil.com"):
                print_red(f"Open Redirect vulnerability detected at {test_url}")
                return
        except requests.RequestException as e:
            st.error(f"Error checking Open Redirect for {url}: {e}")
    print_green(f"No Open Redirect vulnerability detected at {url}")

def check_csrf(url):
    """
    Check for the presence of CSRF tokens in forms.
    """
    try:
        options = webdriver.EdgeOptions()
        options.add_argument('--headless')
        driver = webdriver.Edge(service=EdgeService(EdgeChromiumDriverManager().install()), options=options)
        driver.get(url)
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        driver.quit()

        forms = soup.find_all('form')
        if not forms:
            st.write(f"No forms found on {url}.")
            return

        csrf_protected = True
        for form in forms:
            # Assuming the CSRF token is in a hidden input field
            if not form.find('input', {'type': 'hidden', 'name': 'csrf_token'}):
                csrf_protected = False
                break

        if csrf_protected:
            print_green(f"All forms on {url} are protected with CSRF tokens.")
        else:
            print_red(f"Potential CSRF vulnerability detected on {url}: Some forms do not contain CSRF tokens.")

    except Exception as e:
        st.error(f"Error checking CSRF for {url}: {e}")
        
def check_ssrf(url):
    # This is a basic example; SSRF detection can be complex and might require specific test setups
    test_url = f"{url}?url=http://example.com"
    response = requests.get(test_url)
    if "example.com" in response.text:
        print_red(f"SSRF vulnerability detected at {test_url}")
    else:
        print_green(f"No SSRF vulnerability detected at {test_url}")

def check_cors(url):
    """
    Check for CORS misconfiguration.
    """
    try:
        response = requests.get(url)
        # Check if the Access-Control-Allow-Origin header is present and too permissive
        ac_allow_origin = response.headers.get('Access-Control-Allow-Origin')
        if ac_allow_origin == '*':
            print_red(f"CORS misconfiguration detected: {url} allows all domains.")
        elif ac_allow_origin:
            print_green(f"CORS configuration detected: {url} allows {ac_allow_origin}.")
        else:
            print_green(f"No CORS headers present, default same-origin policy in place for {url}.")
    except requests.RequestException as e:
        st.error(f"Error checking CORS for {url}: {e}")

def main():
    st.title("Vulnerability Scanner")
    base_url = st.text_input("Enter the base URL to scan for vulnerabilities:")
    if st.button("Scan"):
        if base_url:
            links = crawl(base_url)
            st.write(f"Found {len(links)} links to scan.")
            for link in links:
                st.write(f"Scanning {link}...")
                check_sql_injection(link)
                check_xss(link)
                check_open_redirect(link)
                check_csrf(link)
                check_ssrf(link)
                check_cors(link)

if __name__ == "__main__":
    main()
